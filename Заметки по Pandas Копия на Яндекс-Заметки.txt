
Установка Python3 (с сайта python.org), при установке поставить галочку "установить PATH"). 
После установки проверить версию через CMD: python --version

Как найти рабоую папку с интерпретатором Python:
c:> where python

Эмулятор CMD - Conemu

Создание виртуального окружения Python:
C:\WorkPython> python -m venv venv_test
(в рабочей папке будет создана папка venv_test)
Активация виртуального окружения:
venv_test\Scripts\activate
в начале строки в CMD  должно появиться 
(venv_test) C:\WorkPython>
Его деактивация: C:\WorkTest> deactivate

Можно установить JupiterNotebook:
pip install notebook
После его установки можно его запускать из командной строки:
C:\WorkTest> jupyter notebook
(если автоматически не откроется, то надо скопировать в браузер url из CMD)

Второй вариант установки среды для Python - 
это установка дистрибутива Анаконды (найти через браузер "Anaconda Python", скачать и установить дистрибутив). Потом можно запустить Anaconda-Navigator, после открытия которого можно выбрать и запустить JupyterNotebook (но Navigator Жрет много ресурсов).
Рекомендуется запустить Anaconda-Promt (аналог CMD), и там набрать и запустить "jupyter notebook". При этом сразу запускается среда виртуализации для проекта Python.

Список всех горячих клавиш Jupyter:
вкладка HELP -> Keyboard ShortCuts
или нажать "H"

import pandas as pd #импорт библиотеки
df = pd.read_cvs('data_file.cvs') #создание переменной df, в которую мы загрузили данные.

df.head(20) или df.tail(10) показать первые или последние строки датафрейма.
Можно вывести одну колонку с именем 'Product':
df.Product или df['Product']
Если надо вывести более одного столбца:
df[['Product', 'Count', 'Price']] - обратите внимание на двойные скобки
Можно сразу перемножить столбцы:
df['Total'] = df['Product']*df['Count']

Переименование колонок:
df.rename (columns={
	'Old Name1': 'New_Name1',
	'Old Name2': 'New_Name2',
	'Old Name3': 'New_Name3',
	'Old Name4': 'New_Name4'
}, inplace=True)
Если не указывать inplace=True, то начальный датафрейм не будет перезаписан (по умолчанию inplace=False).

Вариант чтения данных с Web-страницы:
df_curr = pd.read_html('https://www.cbr.ru/currency_base/daily')

Сохранение датафреймов:
df.to_cvs('name_new_file.cvs')
df.to_cvs('name_new_file.cvs', index=False) - второй вариант сохранения убирает лишний столбец индексов, автоматически создаваемый при создании датафрейма.

df_test_backup = df_test.copy() # это создаст копию датафрейма, отделенную по связям от исходного фрейма.
Если использовать просто создание нового фрейма:
df_test_backup = df_test  # новый датафрейм backup будет изменять весте с df_test

Выборка(фильтр) строк по условию "цена больше 1000):
df[ df.price>1000 ] #для числовых данных
Выбрать те строки, где в столбце Product содержится LG: 
df[ df.Product.str.contains('LG') ]
# str.contains('LG') - это строковый метод, позволяющий выбрать элементы, в которых содержться подстрока LG.
df[ `df.Product.str.contains('LG') ] # знак тильды инвертирует выбор
Можно использовать составные фильтры:
df[ (df.price>1000) & (df.Product.str.contains('LG')) ]
Вместо прямой записи условий, можно создаваьб отдельные объекты (маски), например:
>mask1 = df.price>1000
>mask2 = (df.Product.str.contains('LG'))
>df[mask1 & mask2]
Эти три строки аналогичны предыдущей команде.

Вывод столбцов:
получение списка столбцов методом .columns: df_test.columns #можно использовать потом цикл: 
for i in df_test: print(i)
df_test['country'].head() или df_test.country.tail()
Предположим, что вы хотите вывести только колонки «country» и «user_id»:
df_test[['country', 'user_id']]
Есть предположения, почему здесь понадобились двойные квадратные скобки? Это может показаться сложным, но, возможно, так удастся запомнить: внешние скобки сообщают pandas, что вы хотите выбрать колонки, а внутренние — список (помните? Списки в Python указываются в квадратных скобках) имен колонок.
Без дополнительных скобок можно вывести только один столец - df_test['country'].

Вывод строк через "срезы":
df_test[20:4:115] # выведет каждую четвертую строку с 20 по 114
df_test[20:4:115]

Узнать размер DF: df1.shape #вывод будет в виде "(5, 150)"
Кол-во строк: df1.shape[0]
Кол-во столцов: df1.shape[1]
Метод df1.loc[n1:n2] - выбирает из DF строки с n1 по n2-1
Итого:
for i in range (0,df2.shape[0],5):
    tmp = df2.loc[i:i+4]
    tmp.to_csv(f'test{i}.csv')
этот цикл создаст несколько DF с именами test0.csv,test1.csv, test2.csv ..., кол-во которых зависит от кол-ва строк в ВDF с именем df2.
Возможности метода loc:
df_test.loc[5:4:115, 'Column5':'Column8':3] #Выведет каждую четвертую строку с 5 по 114, из каждого третьего столбца с 'Column5'по 'Column7'.
df_test.iloc[[1,3,6,8], [3,5,7]  # метод .loc работает с именами столцов, а .iloc - с индексами столбцов. 

df_group1 = df_test.groupby(['Column1']).count()
создаст новый DF, где индексами будет являться значения из Column1, а в столбцах - количество заполненных значений по другим столбцам.
Можно выполнять последовательную группировку по двум столцам:
df_group1 = df_test.groupby(['Column1', 'Column3']).count()

Вставка (вставить) столбца:
df_test['New_col'] = 55 # Простая вставка столбца New_col (значения во всех строках равно 55) в конец таблицы.
df_test['New_col'] = df_test['Col_1'] - df_test['Col_2'] # Простая вставка столбца New_col (значения равны разнице значений между колонками 1 и 2) в конец таблицы.
df_test.insert(0,'Region', 'ErkaDV') # вставка столбца на "0"-ое место с именем Region' и значением по умолчанию 'ErkaDV'
df_test.assing(Region, 'ErkaDV') # вставка столбца на "0"-ое место с именем Region' и значением по умолчанию 'ErkaDV'
Надо понимать, что между функциями assign и insert есть существенное различие.
Функция вставки (insert) работает на месте. Это означает, что изменение (добавление нового столбца) сохраняется во фрейме данных.
С функцией назначения ситуация немного иная. Он возвращает измененный фрейм данных, но не изменяет исходный. Чтобы использовать измененную версию (с новым столбцом), нам нужно явно назначить ее.

Замена отдельных значений в датафрейме:
df_test.loc[5, 'Column1'] = 'New values'
df_test.loc[5:7, 'Column1':'Column3'] = 'New values'
df_test.iloc[5:7, 45:57] = 'New values'
Замена значений с условием:
df_test.loc['df_test.Column1' == 'XXX','Column4'] = 'New values'  #такая команда ищет строки, где в столбце Column1 стоит значение 'XXX' и заменяет в этой строке в столбце Column4 на новое значение.
Можно сразу искать и заменять в одном и том же столбце:
df_test.loc['df_test.Column1' < 100,'Column1'] = 200
тогда все значения в столбце Column1, которые меньше 100, будут заменены на число 200.

Объединение датафреймов:
Рассмотрим случай объединения DF, расположенных в папке 'parts':
import os #Для работы с папками необходима библиотека 'os': 
df_all = pd.DataFrame() #Создадим пустой DF под наванием df_all, в который будем складывать остальные DF
Далее напишем цикл, используя метод pandas 'pd.concat()', который объединяет однотипные DF, например, расположенные в папке  'parts':
for file in os.listdir('parts/'):
    tmp = pd.read_csv('parts/' + file)
    df_all = pd.concat([df_all, tmp])
print(df_all)
В полученном df_all "сбиваются" индексы, для их приведения используется метод df.reset_index c с аргументами inplace=True (перезаписыает индексы с сохранением в этом DF) и drop=True (удаляет старые индексы). 
df_all.reset_index(inplace=True, drop=True)

Работа с датой и временем в Pandas:
Используя метод df.info() можно узнать имена столбцов, кол-во строк в них и тип данных.
Запрос краткого описания датафрейма:
df_test.describe()   #информация о числовых столбцах
df_test.describe(include=['0'])   #информация о текстовых (object) столбцах

Преобразование типа данных object в дата-время:
для формата, изначально заданного как: 12/25/99 15:35:
df_test['Имя столбца'] = pd.to_datatime (df_test.Имя_столбца, format='%m/%d/%y %H:%M')
использование форматов надо искать на странице Pandas'a:
https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior

Столбец index можно заменить на другой, например, на столбец Дата: df1 = df1.set_index('Date')

Использование набора методов и атрибутов, начинающихся с ".dt.":
df.data_sale.dt.month - по каждой дате создаст день название месяца (month - это атрибут данных типа дата)
df.data_sale.dt.day_name() - по каждой дате создаст день название дня недели (day_name()- это метод для данных типа дата)
Соответтсвенно следующие команды создадут доп.столбец:
df_test['Name_month'] = df.data_sale.dt.month 
df_test['День_недели'] = df.data_sale.dt.day_name()

Группировка и аггрегирование данных:
df_test.groupby('День_недели') #по этой команде Pandas создаст семь датафреймов: Monday, Thursday, ..., с которыми можно рабтать, например, получить сумму по каждому фрейму:
df_test.groupby('День_недели').sum() #будут проссумированы все столбцы с числовыми типами данных.
df_test.groupby('День_недели').sum() ['Total'] #будет проссумирован столбец 'Total'.
Можно использвать одновременно несколько аггрегирующих функций:
df_test.groupby('День_недели').agg(['sum', 'count'])['Total'] #выведет семь строк из трех столбцов (первый столбец индексов это "понедельник", второй - сумма продаж по пн, третий - кол-во продаж по пн...)

Метод 'plot' включен в библиотеку Pandas, но лучше импортировать библиотеку MatPlotLib:
import matplotlib
но обычно остаточно импортировать только метод pyplot:
import matplotlib.pyplot as plt #plt стандартный псевдоним pyplot
res = df_test.groupby('День_недели').agg(['sum', 'count'])['Total']  # сохраним данныев новый DF
Существует метод для создания водной таблице .pivot():
df_pivot = df.pivot_table(values='val', index='location', columns='cause', aggfunc='mean',margins=False, dropna=True, fill_value=None) # сводная таблица
Столбчатая диаграмма:
plt.bar(res.index, res['count'])


Четкая выборка из лекции iStories:
df.shape # показывает, сколько в датафрейме строк и столбцов
df.describe()  # статистическая информация по столбцам (уникальные значения, среднее, стандартное отклонение, минимальное, квартили, максимальное)
df.describe(include = 'all') # статистика включает не только числовые столбцы, но и строки 
# (unique - сколько уникальных значений, top - какое самое популярное значение, freq - как часто встречается самое популярное) 
df.dropna(inplace=True) # удаление пустых значений (missing values, NA)
df["measure"] # выбор столбца, способ 1
df.measure # выбор столбца, способ 2[["location","sex","year"]] # выбор сразу нескольких столбцов
df.loc[:, "location":"val"] # выбираем все строки, а столбцы от "location" до "val"
df.loc[100:110, "location":"val"] 
# выбираем строки от 100 до 110 (правый край, то есть число 110, в методе iloc включается), столбцы от "location" до "val"
df.iloc[100:110, 0:3] 
# выбираем строки от 100 до 109 (правый край, то есть число 110, в методе iloc не включается), столбцы от 0 до 3
df[df["sex"] == "Both"] # выбираем по условию только те строки в столбце "sex", где указаны оба пола ("Both")
df[(df["sex"] == "Both") & (df["cause"] == "Cardiovascular diseases") & (df["year"] == 2019)] # несколько условий сразу
df.sort_values(by = ["val"], ascending=False, inplace=False, na_position='last', ignore_index=False)
# сортируем датафрейм по столбцу "val", ascending означает по возрастанию, ascending = False - по убыванию
df[(df["location"] == "Russia") | (df["location"] == "Russian Federation")] 
# ищет строки, где либо Russia, либо Russian Federation (соблюдается одно из условий)
df[df["cause"].str.contains("HIV")] # строки содержат упоминание HIV (ВИЧ)
df[df["cause"].str.contains("HIV") == False] # строки НЕ содержат упоминание ВИЧ
df[~df["cause"].str.contains("HIV")] # строки НЕ содержат упоминание ВИЧ
df.drop("measure", axis=1, inplace = True) # удаляем столбец measure
axis – значение 0, если вы хотите удалить строки, либо 1, если планируете удалять столбцы
df.drop(["metric","age","upper","lower"], axis=1, inplace = True) # удаляем столбцы metric, age, upper, lower
df["val_round"] = df["val"].round(decimals = 1) 
# создаем новый столбец, в котором будут округленные значения из столбца val
df.rename(columns = {"val": "value"}, inplace = True) # меняем названием столбца
df1[df1['Столбец1'].isnull()] #выбор всех строк, где в столбце "Столбец1" стоят пустые NaN
df1[~df1['Столбец1'].isnull()] #выбор всех строк, где в столбце "Столбец1" нет пустых значений NaN
df.to_csv("deaths_2000-2019.csv") # сохранение датафрейма в csv на компьютер

Функция Append добавляет строку в таблицу:
data_pd = data_pd.append({'Фамилия':'Егоров', 'Имя':'Михаил', 'Возраст':'37','Доход':'40000'}, ignore_index=True)
data_pd.head(3) или data_pd[:3] - Выводит указанное количество строк с начала таблицы, в данном случае 3.
data_pd[-3:] - Получаем указанное количество строк с конца таблицы.
data_pd.columns - Выводит названия столбцов в таблице. 
data_pd.dtypes - Выводит тип данных в столбцах.
data_pd[['Фамилия', 'Имя']] - Получаем данные только по отдельным столбцам, а не по всей таблице в целом.
data_pd.loc[[0, 1], ['Фамилия', 'Имя']]  или data_pd.iloc[[0, 1], [0, 1]] - Функция loc и iloc позволяет получить данные только по конкретным строкам и столбцам. Первым агрументом указываем номера строк, которых хотим получить, вторым - названия столбцов.
Отбираем данные по двум условиям:
data_pd[(data_pd['Доход'] >= 30000) & (data_pd['Фамилия'] == 'Петров')] 
Отбирать можно с использованием метода .isin:
df_test[(df_test['Фамилия'].isin(['Петров', 'Иванов','Сидоров'])] #ВЫберет все строки, в которых в столце "Фамилия" стоит либо Иванов, либо Сиборов, либо Петров.

drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise'), где:
labels – номера или названия столбцов для удаления
axis – значение 0, если вы хотите удалить строки, либо 1, если планируете удалять столбцы
index – определяет, какие строки надо удалить
columns – определяет, какие столбцы надо удалить
inplace – изменяет оригинальный Dataframe, если параметр равен True
errors – игнорируются ошибки, если параметр задан как ignore

Глоссарий
pd.DataFrame(данные, columns = [колонки, если есть], index = [индексы ,если есть]) - создать датафрейм
pd.read_csv(полный адрес расположения файла) - открыть .csv файл
.head() - посмотреть верхушку датафрейма (первые n строк)
.tail() - посмотреть конец датафрейма (последние n строк)
.columns - список колонок датафрейма
.values - вывести массив всех значений датафрейма
.index - список индексов датафрейма
.tolist() - перевести в список
.count() - посчитать количество определенных величин во фрейме
.describe() - посмотреть основные статистические характеристики фрейма
.shape - форма фрейма (строки, колонки)
.size - размер фрейма строки*колонки
.info() - информация о данных каждой колонки
.dtypes - тип данных каждой колонки
.isnull() - где недостает значений
.isna()- есть ли значения None
.dropna() - выкинуть строки/колонки с None
.fillna() - заполнить заданным значеним ячейки, где есть None
.loc[] - вывести значения по названиям колонок
.iloc[] - вывести значения по индексам колонок
.drop() - выкинуть определенные значения
pd.to_datetime(колонка, которую переводим в формат временного ряда)
.groupby() - сгруппировать по конкретному признаку
.copy() - создать копию
.sort_values() - сортировка значений
pd.concat([df1,df2]) - конкатенация фреймов
.merge(второй_датафрейм, on = 'общая колонка, по которой склеиваем', how = 'с какой стороны') - конкатенация фреймов через общий признак
.corr() - вычислить корреляцию
.median() - вычислить медиану
.cumsum() - вычислить куммулятивную сумму
.cumprod() - вычислить коммулятивное произведение
.cummax() - вычислить коммулятивный максимум
.quantile([]) - вычислить квантили
.nunique() - уникальные значения для n-колонок/строк
.unique() - уникальные значения определенной колонки/строк
.apply(функция) - применить функцию для колонки/строки
.agg(набор_функций) - применить ряд функций для колонки/строки

df["val"].mean() # среднее по одному столбцу
df['val'].max() # максимальное значение по одному столбцу
df['val'].min() # минимальное значение по одному столбцу
df['cause'].count() # число записей
df['cause'].unique() # уникальные значения
df['cause'].value_counts() # число записей соответствующих уникальным значениям
df.sort_values(['val'], ascending = False) # сортировка
df.nlargest(5,'val') # 5 наибольших значений по столбцу 'val'
df[df['year'] == 2019].nlargest(5,'val') # 5 наибольших значений по столбцу 'val' для 2019 года
df[(df['location'].isin(['Russian Federation', 'Ukraine', 'Belarus'])) & (df['year'] == 2019)].nlargest(5,'val')
# 5 наибольших значений по столбцу 'val' для 2019 года для стран Россия, Украина и Белоруссия


Перебор фалой в папке:
import os
df_all = pd.DataFrame()
for f in os.listdir('parts/'):
#     print(f)
    tmp = pd.read_csv('parts/' + f)
    df_all = pd.concat([df_all, tmp])
    
df_all

#df = df.applymap(str) #все данные в датафрейме превращаем в строковые

pass - пустая функция или цикл

df_test.iloc[1,:]  - такая запись выдаст объект Series
df_test.iloc[1:2,:]  - такая запись выдаст объект DataFrame
df_test.tolist() - преобразует объект Series в список

Вызов .tolist() не обновит вашу структуру на месте. Вместо этого метод вернет новый список без изменения исходного объекта pd.Series. Это означает, что мы должны присвоить результат исходной переменной, чтобы обновить его. Однако, если исходная переменная является фрагментом pd.DataFrame(), мы не можем этого сделать, поскольку DataFrame автоматически преобразует list в pd.Series при назначении.
Это означает, что выполнение numbers[2] = numbers[2].tolist() будет по-прежнему numbers[2] быть pd.Series. Чтобы получить список, нам нужно присвоить вывод другой (возможно, новой) переменной, которая не является частью DataFrame.

df = df.iloc[:,:10]     #срезать лишние столбцы
df = df[~df['End_FN'].isnull()] # удаление строки, где в слобце 'Срок окончания действия ФН' нет значений,
                    # т.е. стоит Nan
df['End_FN'] = pd.to_datetime(df.End_FN, format='%d.%m.%Y %H:%M:%S')   #данные столбца End_FN в ДатаВремя
df['Date_KKM'] = pd.to_datetime(df.Date_KKM, format='%d.%m.%Y %H:%M:%S')  #данные столбца Date_KKM в ДатаВремя

df.reset_index(drop=True, inplace=True)    # После соединения DF и удаления строк сбиваются индексы,данная команда проставляет их заново

date1 = datetime.datetime.now()
date2 = date1 + datetime.timedelta(days=20)
#date_str = '2021-01-29 00:00:00.000000'
#date3 = datetime.datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S.%f')

if __name__ == '__main__': main()
Данная конструкция нормально позволяет работать при экспорте файлов *.py, так как при импорте файла test2.py в другой файл test1.py выполняется весь код из файла test2.py, но при этом внутренняя переменная __name__ становиться равной 'test1.py' (из которого был произведен импорт). Чтобы этого избежать,весь код импортируемого файла надо обернуть в функция main()и вставить следующую конструкцию:
if __name__ == '__main__': main()
Переменная __name__ принимает значение '__main__' только когда файл запускается сам по себе, без вызова из другого *.py файла.

пустая функция - pass

Обработка ошибок:
try:
    # В этом блоке могут быть ошибки
except <error type>:
    # Сделай это для обработки исключения;
    # выполняется, если блок try выбрасывает ошибку
else:
    # Сделай это, если блок try выполняется успешно, без ошибок
finally:
    # Этот блок выполняется всегда

Для проверки существования заданного пути используйте функцию:
import os.path
os.path.exists(file_path)
Но она вернет True и для файла и для директории.
os.path.isfile(file_path) проверит именно на наличие файла.

C:\temp\proga> python Main_proga.py - таким образом можно запустить питоновский файл из CMD